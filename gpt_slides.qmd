---
title: "IP-weighting and Marginal Structural Models (MSMs)"
author: ""
date: today
date-format: long
format: baylor_theme-revealjs
footer: "Nathen Byford"
---

```{r}
#| include: false

library(tidyverse); theme_set(theme_bw())
library(causaldata)

dat <- nhefs

nhefs.nmv <- nhefs[which(!is.na(nhefs$wt82)),]
```


# Introduction


## Motivating Example

:::{.def}
We are interested in the question "What is the average causal effect of smoking cessation on body weight gain?"
:::

- Using data from NHEFS study from 1971-1982 with a baseline visit and a follow-up visit
    - Subjects are considered treated $A = 1$ if they quit smoking before the follow-up
    
- Only individuals with known age, sex, race, weight, height, education, alcohol use and intensity of smoking at baseline and followup are considered.

## Example Data

![](mean_baseline.png){fig-align='center'}

Individuals are classified as treated $A=1$ based on two conditions:

(a) If they reported being smokers at baseline, and
(b) If they have reported to quit smoking in 1982.

:::{.callout-warning}
## Selection Bias

Bias may be induced by condition (b) due to potential selection into/out of the study.
:::

```{r}
#| echo: false
#| eval: false
#| tbl-cap: "Mean Baseline Characteristics"

dat |> 
  mutate(
    "trt" = qsmk,
    "University" = education == 5,
    "Men" = sex == 0,
    "White" = race == 0
    ) |> 
  select(
    qsmk, age, Men, White, University, wt71, smokeintensity,
    smokeyrs
  ) |> 
  gtsummary::tbl_summary(by = "trt")


```



# Inverse Probability Weighting


## Inverse Probability Weighting: Overview

- A method of creating a pseudo-population that uses _weights_ to account for confounding.
- Each individual is weighted by the inverse of the **probability of receiving the treatment they actually received**.

$$
\operatorname{Weight} = \frac{1}{\widehat{\text{Pr}}(A \mid L)}
$$


## IPW: Computation

- Goal: Estimate causal effect of treatment
- Use the pseudo-population to estimate the difference by 
$$
\widehat{E}_{\text{ps}}[Y|A=1] - \widehat{E}_{\text{ps}}[Y|A=0]
$$
- Estimate pseudo population difference with weighted least squares
$$
E[Y|A] = \theta_0 + \theta_1A
$$
    - Weight each individual by their estimated IP weights
    - The parameter estimate $\hat{\theta}_1$ is the estimated treatment effect

## IPW: Assumptions

- **No unmeasured confounding**: All confounders must be measured.
- **Correct specification of the propensity score model**: The model for treatment assignment must be correct.
- **Positivity**: There must be non-zero probability of receiving each treatment for all levels of confounders.
- **Consistency**: Observed outcomes must correspond to the treatment actually received.
- **Stable Unit Treatment Value Assumption (SUTVA)**: There is no interference between individuals (i.e., one person’s treatment does not affect another's outcome).



## IPW: Rationale

![](balance.png){fig-align="center"}

1. In observational studies, treatment assignment isn't random
2. Give more weight to "surprising" observations and less weight to "expected" observations
3. Build a pseudo-population where the treatment looks more randomized

## Horvitz-Thompson estimators{.smaller}

The IP weighted mean is consistantly estimated by the Horvitz-Thomson estimator
$$
\hat{E}\left[\frac{I(A=a)Y}{f(A|L)}\right]
$$

When estimating $E[Y^a]$ via IP weighting with least squares for a binary $A$, the modified Horvitz-Thompson estimator refered to as the Hajek estimator is used.
$$
\frac{\hat{E}\left[\frac{I(A=a)Y}{f(A|L)}\right]}{\hat{E}\left[\frac{I(A=a)}{f(A|L)}\right]}
$$

## IPW: Pros and Cons

- **Advantages:**
  - More **robust to model misspecification** of the outcome model.
  - Avoids directly modeling the outcome.

- **Disadvantages:**
  - Requires correct specification of the **propensity score model**.
  - **Extreme weights** can lead to instability and inefficiency.
  - The size of the pseudo-population is twice the size of the study population


## IPW: Stabilized weights

Stabilized weights:
$$\operatorname{SW}_i = \frac{\text{Pr}(A_i)}{\text{Pr}(A_i|L_i)} = \frac{f(A)}{f(A|L)}$$

Key Properties

- Numerator: Marginal probability of treatment
- Denominator: Conditional probability of treatment
- Expected value of stabilized weights approximately 1 (Pseudo-population size equal to sample population)

## IWP: Example {.smaller}

1. **Estimate Propensity Score:** Using logistic regression

```{r}
#| echo: true
#| code-fold: true

fit <- glm(
  qsmk ~ sex + race + age + I(age ^ 2) +
    as.factor(education) + smokeintensity +
    I(smokeintensity ^ 2) + smokeyrs + I(smokeyrs ^ 2) +
    as.factor(exercise) + as.factor(active) + wt71 + I(wt71 ^ 2),
  family = binomial(),
  data = nhefs.nmv
)

```

2. **Calculate Weights:** Non-stabilized weights

```{r}
#| echo: true
#| code-fold: true

p.qsmk.obs <-
  ifelse(nhefs.nmv$qsmk == 0,
         1 - predict(fit, type = "response"),
         predict(fit, type = "response"))

nhefs.nmv$w <- 1 / p.qsmk.obs

summary(nhefs.nmv$w)
```


3. **Weighted Least Squares Model:**

```{r}
#| echo: true
#| code-fold: true

library("geepack")

msm.w <- geeglm(
  wt82_71 ~ qsmk,
  data = nhefs.nmv,
  weights = w,
  id = seqn,
  corstr = "independence"
)

gtsummary::tbl_regression(msm.w, intercept = TRUE)
```


## IWP: Example (Stabilized weights) {.smaller}

1. **Estimate Propensity Score:** Using logistic regression

```{r}
#| echo: true
#| code-fold: true

denom.fit <-
  glm(
    qsmk ~ as.factor(sex) + as.factor(race) + age + I(age ^ 2) +
      as.factor(education) + smokeintensity +
      I(smokeintensity ^ 2) + smokeyrs + I(smokeyrs ^ 2) +
      as.factor(exercise) + as.factor(active) + wt71 + I(wt71 ^ 2),
    family = binomial(),
    data = nhefs.nmv
  )

pd.qsmk <- predict(denom.fit, type = "response")
```

2. **Calculate Weights:** Stabilized weights

```{r}
#| echo: true
#| code-fold: true

numer.fit <- glm(qsmk ~ 1, family = binomial(), data = nhefs.nmv)

pn.qsmk <- predict(numer.fit, type = "response")

nhefs.nmv$sw <-
  ifelse(nhefs.nmv$qsmk == 0, ((1 - pn.qsmk) / (1 - pd.qsmk)),
         (pn.qsmk / pd.qsmk))

summary(nhefs.nmv$sw)
```

3. **Weighted Least Squares Model:**

```{r}
#| echo: true
#| code-fold: true

msm.sw <- geeglm(
  wt82_71 ~ qsmk,
  data = nhefs.nmv,
  weights = sw,
  id = seqn,
  corstr = "independence"
)

gtsummary::tbl_regression(msm.sw, intercept=TRUE)
```

## IWP: Example Discussion

With both stabilized and non-stabilized weights the estimated treatment effect is found to be $\hat{\theta}_1 = 3.4$.

- Why should we bother using the stabilized model if the results are the same?
    - Typically the stabilized weights results in a narrower 95\% CI
    - The benefits can only really be realized when the model is not saturated
    - Noticed in many settings such as continuous treatments or time-varying treatments


# Marginal Structural Models



## Marginal Structural Models (MSMs): Overview

- Developed to handle **continuous treatments**.
- Combine **IPW** and **structural models** to estimate causal effects.
- MSMs model the effect of treatment on outcome, weighted by IPW to adjust for confounders.

Consider the following linear model
$$
E[Y^a] = \beta_0 + \beta_1a
$$

:::{.subsubtext}
Models for the marginal mean of a counterfactual outcome are referred to as *marginal structural mean models*
:::

## MSMs: Example

1. **Step 1: Calculate Stabilized Weights**
   $$
   SW = \frac{P(Treatment)}{P(Treatment \mid Confounders)}
   $$
   These weights are used to reduce variance while accounting for confounding.

2. **Step 2: Fit MSM**
   $$
   Y = \beta_0 + \beta_1 \times Treatment
   $$
   Weighted regression with **stabilized weights**.



## MSMs: Key Strengths

- **Handles time-varying treatments** and confounders.
- Suitable for longitudinal data and **dynamic treatment regimes**.
- Can capture **causal effects** that change over time.



## Comparing Methods {.smaller}

| Method                 | Key Idea                             | Pros                                         | Cons                                       |
|------------------------|--------------------------------------|----------------------------------------------|--------------------------------------------|
| **G-computation**       | Model counterfactual outcomes        | Directly estimates effects                   | Model misspecification is critical         |
| **IP-weighting**        | Re-weight to balance confounders     | Avoids modeling the outcome                  | Sensitive to extreme weights               |
| **MSMs**               | Combine IPW with structural models   | Handles time-varying treatments              | Complex to implement, requires weights     |



## Conclusion

- **G-computation**: Direct modeling of potential outcomes.
- **IP-weighting**: Uses weights to adjust for confounders.
- **MSMs**: Combines IP-weighting and models for complex settings.
- Choosing the right method depends on the context (e.g., time-varying treatments, data structure).



## References

- Robins, J.M. (1986). "A new approach to causal inference in mortality studies."
- Hernán, M.A., & Robins, J.M. (2020). *Causal Inference: What If.*
- VanderWeele, T.J. (2015). *Explanation in Causal Inference: Methods for Mediation and Interaction.*
- Slide help from chatGPT

